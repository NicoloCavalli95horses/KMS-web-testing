---
ID: 2025-03-24-13:10
tags:
  - "#definition"
  - algorithm
  - Http
---
## Definition

Traces are the HTTP requests generated during navigation of the application under attack-free sessions. These traces can be generated either manually or automatically. The traces are analyzed for extraction of the intended behavior of the application

### Automatic approach ( web crawler)

**Benefit**
- automatic, fast, less error prone

**Challenges of automatic trace collection (crawler)**
- possible false behavior may be inferred
- possibility of missing few vulnerable pages from being identified by the crawler as the crawler may not navigate through the application in a fashion intended by the programmer
- difficult to implement a context-aware crawler for workflow extraction: a crawler does not know what is the best way to get from a page to another

Manual traces are generated by allowing a tester to navigate through the application in a browser configured to use a proxy server. The proxy intercepts the HTTP requests/responses which are stored for further analysis


---
#### References
- [[(Deepa, Thilagam, et al., 2018)]]